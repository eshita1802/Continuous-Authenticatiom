{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshita1802/Continuous-Authentication/blob/main/DeepConvLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFFT7NE_r210",
        "outputId": "94b73e31-ccbf-42c6-bc97-cc274a939d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas tqdm openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSNJ8CHetLry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa4d0ad-d6a8-4aef-c935-019b34efeea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ldjcDtFGtWxX",
        "outputId": "53ca7c00-1fd9-445b-a89b-8b2a4a960269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 218_WattsUp_2023.pdf\t\t  'Resume with updated project.gdoc'\n",
            "'21je0334_eshita Paliwal_md.pdf'   Screenshot_20241015_182553.jpg\n",
            "'21JE0334_Eshita Paliwal.pdf'\t  'Segmented Dataset'\n",
            "'Alumni Outreach .gdoc'\t\t   SOFTCOMPUTING-PROJECT-2024-25.gdoc\n",
            "'Alumni Outreach.gsheet'\t  'SoP Eshita Paliwal.gdoc'\n",
            "'Colab Notebooks'\t\t  'SoP Eshita Paliwal.pdf'\n",
            "'Copy of Sakshi IIT Resume.gdoc'  'Speak Up Sessions :- Rough Draft.gdoc'\n",
            "'Dataset New'\t\t\t  'Startups Alumni.gsheet'\n",
            "'Eshita Paliwal.png'\t\t  'Switch Energy Case.gdoc'\n",
            "'Eshita paliwal_Resume.pdf'\t   team-218.pdf.pdf\n",
            "'Resume -Eshita Paliwal.gdoc'\t  'WhatsApp Image 2022-08-01 at 10.23.20 PM (1).jpeg'\n",
            " Resume.pdf\t\t\t  'WhatsApp Image 2022-08-01 at 10.23.20 PM.jpeg'\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/My Drive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvjvflKftep0"
      },
      "outputs": [],
      "source": [
        "input_folder = '/content/drive/My Drive/Security of Smart Phone'  # Path to your data\n",
        "output_folder = '/content/drive/My Drive/Dataset New'  # Path to save the output files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsyEHNLIti-r",
        "outputId": "9e2018e9-f4ef-4f95-87ab-1312601fd3e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing Activities:   0%|          | 0/7 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Biking - left_pocket\n",
            "Processing Biking - right_pocket\n",
            "Processing Biking - wrist\n",
            "Processing Biking - upper_arm\n",
            "Processing Biking - belt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing Activities:  14%|█▍        | 1/7 [16:45<1:40:35, 1005.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Running - left_pocket\n",
            "Processing Running - right_pocket\n",
            "Processing Running - wrist\n",
            "Processing Running - upper_arm\n",
            "Processing Running - belt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing Activities:  29%|██▊       | 2/7 [33:30<1:23:46, 1005.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Sitting - left_pocket\n",
            "Processing Sitting - right_pocket\n",
            "Processing Sitting - wrist\n",
            "Processing Sitting - upper_arm\n",
            "Processing Sitting - belt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing Activities:  43%|████▎     | 3/7 [50:18<1:07:05, 1006.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Standing - left_pocket\n",
            "Processing Standing - right_pocket\n",
            "Processing Standing - wrist\n",
            "Processing Standing - upper_arm\n",
            "Processing Standing - belt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing Activities:  57%|█████▋    | 4/7 [1:07:04<50:18, 1006.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Walking - left_pocket\n",
            "Processing Walking - right_pocket\n",
            "Processing Walking - wrist\n",
            "Processing Walking - upper_arm\n",
            "Processing Walking - belt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing Activities:  71%|███████▏  | 5/7 [1:23:48<33:30, 1005.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Walking Upstairs - left_pocket\n",
            "Processing Walking Upstairs - right_pocket\n",
            "Processing Walking Upstairs - wrist\n",
            "Processing Walking Upstairs - upper_arm\n",
            "Processing Walking Upstairs - belt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing Activities:  86%|████████▌ | 6/7 [1:40:38<16:46, 1006.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Walking Downstairs - left_pocket\n",
            "Processing Walking Downstairs - right_pocket\n",
            "Processing Walking Downstairs - wrist\n",
            "Processing Walking Downstairs - upper_arm\n",
            "Processing Walking Downstairs - belt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Activities: 100%|██████████| 7/7 [1:57:24<00:00, 1006.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Excel files created successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "\n",
        "# Define paths and parameters\n",
        "activity_names = [\"Biking\", \"Running\", \"Sitting\", \"Standing\", \"Walking\", \"Walking Upstairs\", \"Walking Downstairs\"]\n",
        "positions = {\n",
        "    \"left_pocket\": (1, 13),\n",
        "    \"right_pocket\": (15, 27),\n",
        "    \"wrist\": (29, 41),\n",
        "    \"upper_arm\": (43, 55),\n",
        "    \"belt\": (57, 69)\n",
        "}\n",
        "input_folder = '/content/drive/My Drive/Security of Smart Phone'  # Folder with activity folders and participant files\n",
        "output_folder = '/content/drive/My Drive/Dataset New'  # Folder for saving 35 Excel files\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Process each activity and position combination with tqdm progress bar\n",
        "for activity in tqdm(activity_names, desc=\"Processing Activities\"):\n",
        "    for position, (start_col, end_col) in positions.items():\n",
        "        all_data = []  # List to collect data for all participants\n",
        "        print(f\"Processing {activity} - {position}\")\n",
        "\n",
        "        # Loop through each participant file\n",
        "        for participant in range(1, 11):\n",
        "            file_path = os.path.join(input_folder, activity, f\"Participant_{participant}.csv\")\n",
        "            df = pd.read_csv(file_path, header=1)\n",
        "\n",
        "            # Select position-specific columns and add participant and activity labels\n",
        "            position_data = df.iloc[:, start_col:end_col]\n",
        "            position_data.insert(0, 'participant_id', participant)  # Insert participant_id at the beginning\n",
        "            position_data.insert(1, 'activity_position', f\"{activity}_{position}\")  # Insert activity_position label\n",
        "\n",
        "            # Append to all_data list\n",
        "            all_data.append(position_data)\n",
        "\n",
        "        # Concatenate all participant data into a single DataFrame\n",
        "        consolidated_df = pd.concat(all_data, axis=0, ignore_index=True)\n",
        "\n",
        "        # Write the consolidated data to an Excel file\n",
        "        output_file = os.path.join(output_folder, f\"{activity}_{position}.xlsx\")\n",
        "        consolidated_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Excel files created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orvfdciTUiLd",
        "outputId": "79a2457a-468b-41f7-d484-e7617ce9c94b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Files: 100%|██████████| 35/35 [1:26:32<00:00, 148.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data segmented and saved for training and testing successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "# Define paths\n",
        "input_folder = '/content/drive/My Drive/Dataset New'  # Folder with the 35 Excel files\n",
        "output_folder = '/content/drive/My Drive/Segmented Dataset'  # Folder to save segmented data\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Segmentation parameters\n",
        "window_size = 250\n",
        "overlap = 0.5\n",
        "step_size = int(window_size * (1 - overlap))\n",
        "\n",
        "# Loop through each Excel file with a progress bar\n",
        "for file_name in tqdm(os.listdir(input_folder), desc=\"Processing Files\"):\n",
        "    if file_name.endswith('.xlsx'):\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "        df = pd.read_excel(file_path)\n",
        "\n",
        "        # Collect segmented data\n",
        "        segments = []\n",
        "\n",
        "        # Create segments\n",
        "        for start in range(0, len(df) - window_size + 1, step_size):\n",
        "            segment = df.iloc[start:start + window_size].values\n",
        "            segments.append(segment)\n",
        "\n",
        "        # Convert to numpy array for easy splitting\n",
        "        segments = np.array(segments)\n",
        "\n",
        "        # Split into training and testing (e.g., 80% train, 20% test)\n",
        "        train_segments, test_segments = train_test_split(segments, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Save segmented data for training and testing\n",
        "        activity_position = file_name.replace('.xlsx', '')  # Extract activity and position from filename\n",
        "\n",
        "        # Save training segments\n",
        "        train_file_path = os.path.join(output_folder, f\"{activity_position}_train.npy\")\n",
        "        np.save(train_file_path, train_segments)\n",
        "\n",
        "        # Save test segments\n",
        "        test_file_path = os.path.join(output_folder, f\"{activity_position}_test.npy\")\n",
        "        np.save(test_file_path, test_segments)\n",
        "\n",
        "print(\"Data segmented and saved for training and testing successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doMr1BDMOqja",
        "outputId": "6a47139c-8d5d-4186-d29d-1bb8c4d75a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 218_WattsUp_2023.pdf\t\t  'Resume with updated project.gdoc'\n",
            "'21je0334_eshita Paliwal_md.pdf'   Screenshot_20241015_182553.jpg\n",
            "'21JE0334_Eshita Paliwal.pdf'\t  'Segmented Dataset'\n",
            "'Alumni Outreach .gdoc'\t\t   SOFTCOMPUTING-PROJECT-2024-25.gdoc\n",
            "'Alumni Outreach.gsheet'\t  'SoP Eshita Paliwal.gdoc'\n",
            "'Colab Notebooks'\t\t  'SoP Eshita Paliwal.pdf'\n",
            "'Copy of Sakshi IIT Resume.gdoc'  'Speak Up Sessions :- Rough Draft.gdoc'\n",
            "'Dataset New'\t\t\t  'Startups Alumni.gsheet'\n",
            "'Eshita Paliwal.png'\t\t  'Switch Energy Case.gdoc'\n",
            "'Eshita paliwal_Resume.pdf'\t   team-218.pdf.pdf\n",
            "'Resume -Eshita Paliwal.gdoc'\t  'WhatsApp Image 2022-08-01 at 10.23.20 PM (1).jpeg'\n",
            " Resume.pdf\t\t\t  'WhatsApp Image 2022-08-01 at 10.23.20 PM.jpeg'\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/My Drive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bhIKfMpWPP6V",
        "outputId": "0efc23f4-1605-46d3-c3ad-fa92a18b0fc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rNormalizing Data:   0%|          | 0/35 [00:00<?, ?file/s]Exception ignored in: <function ZipFile.__del__ at 0x7dc64766e8c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/zipfile.py\", line 1834, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/lib/python3.10/zipfile.py\", line 1851, in close\n",
            "    self.fp.seek(self.start_dir)\n",
            "ValueError: seek of closed file\n",
            "Normalizing Data:  14%|█▍        | 5/35 [30:57<3:05:47, 371.57s/file]\n"
          ]
        },
        {
          "ename": "BadZipFile",
          "evalue": "File is not a zip file",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-cdd3f48a1e6b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Check if the file has already been normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m             \u001b[0;31m# Workaround for some third party files that use forward slashes and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m             \u001b[0;31m# lower case names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define path for the dataset\n",
        "input_folder = '/content/drive/My Drive/Dataset New'  # Folder with the 35 Excel files\n",
        "\n",
        "# Loop through each Excel file with a progress bar\n",
        "for file_name in tqdm(os.listdir(input_folder), desc=\"Normalizing Data\", unit=\"file\"):\n",
        "    if file_name.endswith('.xlsx'):\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "        df = pd.read_excel(file_path)\n",
        "\n",
        "        # Check if the file has already been normalized\n",
        "        if 'normalized' in df.columns:\n",
        "            print(f\"Skipping {file_name}, already normalized.\")\n",
        "            continue\n",
        "\n",
        "        # Separate labels (participant_id and activity_position) from sensor data\n",
        "        labels = df[['participant_id', 'activity_position']]\n",
        "        sensor_data = df.drop(columns=['participant_id', 'activity_position'])  # Only sensor columns\n",
        "\n",
        "        # Apply normalization to the sensor data\n",
        "        scaler = StandardScaler()\n",
        "        normalized_data = scaler.fit_transform(sensor_data)\n",
        "\n",
        "        # Convert normalized data back to DataFrame and concatenate with labels\n",
        "        normalized_df = pd.DataFrame(normalized_data, columns=sensor_data.columns)\n",
        "        final_df = pd.concat([labels, normalized_df], axis=1)\n",
        "\n",
        "        # Add a 'normalized' flag column to mark this file as processed\n",
        "        final_df['normalized'] = True\n",
        "\n",
        "        # Overwrite the original file with normalized data\n",
        "        final_df.to_excel(file_path, index=False)\n",
        "\n",
        "print(\"Normalization complete for all Excel files, with participant_id preserved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcYLNLSRdR5G",
        "outputId": "6e0a1adb-d9f8-4c08-8797-151a3afb5816"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Normalizing Data:  86%|████████▌ | 30/35 [3:13:09<23:53, 286.65s/file]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Running_left_pocket.xlsx due to error: File is not a zip file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rNormalizing Data:  89%|████████▊ | 31/35 [3:16:17<17:07, 256.85s/file]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Biking_left_pocket.xlsx, already normalized.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rNormalizing Data:  91%|█████████▏| 32/35 [3:19:32<11:55, 238.50s/file]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Biking_right_pocket.xlsx, already normalized.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rNormalizing Data:  94%|█████████▍| 33/35 [3:22:46<07:30, 225.24s/file]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Biking_wrist.xlsx, already normalized.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rNormalizing Data:  97%|█████████▋| 34/35 [3:26:05<03:37, 217.08s/file]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Biking_upper_arm.xlsx, already normalized.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Normalizing Data: 100%|██████████| 35/35 [3:29:18<00:00, 358.83s/file]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping Biking_belt.xlsx, already normalized.\n",
            "Normalization complete for all Excel files, with participant_id preserved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define path for the dataset\n",
        "input_folder = '/content/drive/My Drive/Dataset New'  # Folder with the 35 Excel files\n",
        "\n",
        "# Loop through each Excel file with a progress bar\n",
        "for file_name in tqdm(os.listdir(input_folder), desc=\"Normalizing Data\", unit=\"file\"):\n",
        "    if file_name.endswith('.xlsx'):\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "\n",
        "        # Try to load the file and skip it if there's an error\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {file_name} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Check if the file has already been normalized\n",
        "        if 'normalized' in df.columns:\n",
        "            print(f\"Skipping {file_name}, already normalized.\")\n",
        "            continue\n",
        "\n",
        "        # Separate labels (participant_id and activity_position) from sensor data\n",
        "        labels = df[['participant_id', 'activity_position']]\n",
        "        sensor_data = df.drop(columns=['participant_id', 'activity_position'])  # Only sensor columns\n",
        "\n",
        "        # Apply normalization to the sensor data\n",
        "        scaler = StandardScaler()\n",
        "        normalized_data = scaler.fit_transform(sensor_data)\n",
        "\n",
        "        # Convert normalized data back to DataFrame and concatenate with labels\n",
        "        normalized_df = pd.DataFrame(normalized_data, columns=sensor_data.columns)\n",
        "        final_df = pd.concat([labels, normalized_df], axis=1)\n",
        "\n",
        "        # Add a 'normalized' flag column to mark this file as processed\n",
        "        final_df['normalized'] = True\n",
        "\n",
        "        # Overwrite the original file with normalized data\n",
        "        final_df.to_excel(file_path, index=False)\n",
        "\n",
        "print(\"Normalization complete for all Excel files, with participant_id preserved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "kWhhHJO__sRz",
        "outputId": "88ba7cb7-99aa-4216-d57c-4864313bef12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rLabelling Data:   0%|          | 0/35 [00:00<?, ?file/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping file Running_left_pocket.xlsx: Not a valid Excel file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Labelling Data:  69%|██████▊   | 24/35 [2:23:47<1:08:42, 374.81s/file]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define path for the dataset\n",
        "input_folder = '/content/drive/My Drive/Dataset New'  # Folder with normalized Excel files\n",
        "\n",
        "# Loop through each file with progress bar for labeling\n",
        "for file_name in tqdm(os.listdir(input_folder), desc=\"Labelling Data\", unit=\"file\"):\n",
        "    if file_name.endswith('.xlsx'):\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "\n",
        "            # Ensure participant_id and activity_position are present\n",
        "            if 'participant_id' in df.columns and 'activity_position' in df.columns:\n",
        "                # Overwrite the original file with labels added/verified\n",
        "                df.to_excel(file_path, index=False)\n",
        "            else:\n",
        "                print(f\"Labels missing in file: {file_name}\")\n",
        "\n",
        "        except (pd.errors.EmptyDataError, pd.errors.ParserError, ValueError, OSError) as e:\n",
        "            print(f\"Skipping file {file_name} due to read error: {e}\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Skipping file {file_name}: Not a valid Excel file\")\n",
        "\n",
        "print(\"Labelling complete for all files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Custom Data Generator Class\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, folder_path, batch_size=16, is_train=True):\n",
        "        self.folder_path = folder_path\n",
        "        self.batch_size = batch_size\n",
        "        self.is_train = is_train\n",
        "        self.files = [f for f in os.listdir(folder_path) if 'segments' in f and ('train' in f if is_train else 'test' in f)]\n",
        "        self.indices = np.arange(len(self.files))\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        X_batch, y_batch = [], []\n",
        "\n",
        "        for i in batch_files:\n",
        "            segment_file = self.files[i]\n",
        "            label_file = segment_file.replace('segments', 'labels')\n",
        "\n",
        "            # Load data\n",
        "            X = np.load(os.path.join(self.folder_path, segment_file), allow_pickle=True)\n",
        "            y = np.load(os.path.join(self.folder_path, label_file), allow_pickle=True)\n",
        "\n",
        "            # Ensure X is the expected shape\n",
        "            if X.shape[1] != 250:  # Check if segments are of length 250\n",
        "                print(f\"Warning: Skipping file {segment_file} due to unexpected segment length {X.shape[1]}\")\n",
        "                continue\n",
        "\n",
        "            X_batch.append(X)\n",
        "            y_batch.append(y)\n",
        "\n",
        "        X_batch = np.concatenate(X_batch, axis=0)\n",
        "        y_batch = np.concatenate(y_batch, axis=0)\n",
        "\n",
        "        # Reshape and one-hot encode labels\n",
        "        X_batch = X_batch.reshape((X_batch.shape[0], X_batch.shape[1], 1))\n",
        "        y_batch = tf.keras.utils.to_categorical(y_batch)\n",
        "\n",
        "        return X_batch, y_batch\n",
        "\n",
        "# Initialize the Generators\n",
        "folder_path = '/content/drive/My Drive/Segmented Dataset'\n",
        "batch_size = 16  # Adjust batch size\n",
        "\n",
        "train_generator = DataGenerator(folder_path=folder_path, batch_size=batch_size, is_train=True)\n",
        "test_generator = DataGenerator(folder_path=folder_path, batch_size=batch_size, is_train=False)\n",
        "\n",
        "# Determine the number of classes dynamically from the labels\n",
        "all_labels = []\n",
        "for f in os.listdir(folder_path):\n",
        "    if 'labels' in f:\n",
        "        labels = np.load(os.path.join(folder_path, f), allow_pickle=True)\n",
        "        all_labels.extend(labels.tolist())\n",
        "num_classes = len(np.unique(all_labels))\n",
        "\n",
        "# Define the CNN-LSTM Model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(250, 1)),  # Maintain the same input shape\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    LSTM(100, return_sequences=True),  # LSTM expects sequences\n",
        "    Dropout(0.25),\n",
        "    LSTM(50),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.25),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.25),\n",
        "    Dense(num_classes, activation='softmax')  # Use the dynamically determined number of classes\n",
        "])\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the Model Using Data Generators with Progress Bar\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    with tqdm(total=len(train_generator), unit=\"batch\") as pbar:\n",
        "        model.fit(train_generator, epochs=1, validation_data=test_generator, verbose=0)\n",
        "        pbar.update(len(train_generator))\n",
        "\n",
        "# Evaluate the Model on the Test Set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "ex2nF2fkIloZ",
        "outputId": "81fec6d1-fe57-429d-89e1-85383df96197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2 [01:07<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 241860000 into shape (64496,250,1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1e8667ed0127>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1e8667ed0127>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Reshape and one-hot encode labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 241860000 into shape (64496,250,1)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}