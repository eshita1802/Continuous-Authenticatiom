{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwmF01XV81HKRcK1tEkil5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSmwgui7qoN1",
        "outputId": "eb7d97cc-6a7a-4a49-9664-0bcd9e22c302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV without dtype specification\n",
        "df = pd.read_csv('/content/drive/My Drive/subset_participant1_labeled.csv', low_memory=False)\n",
        "\n",
        "# Convert the magnetometer_z column to numeric, setting errors to NaN\n",
        "df['Mz'] = pd.to_numeric(df['Mz'], errors='coerce')\n",
        "\n",
        "# Check if there are any NaN values after conversion\n",
        "nan_count = df['Mz'].isna().sum()\n",
        "print(f\"Number of NaN values in magnetometer_z after conversion: {nan_count}\")\n",
        "\n",
        "# Optionally, drop rows with NaN values or fill them\n",
        "df.dropna(subset=['Mz'], inplace=True)  # Drop rows with NaN in magnetometer_z\n",
        "# or alternatively:\n",
        "# df['magnetometer_z'].fillna(0, inplace=True)  # Fill NaNs with 0\n",
        "\n",
        "# Check rows where magnetometer_z is not numeric\n",
        "invalid_rows = df[~df['Mz'].apply(lambda x: isinstance(x, (int, float)))]\n",
        "print(f\"Number of invalid rows in 'Mz': {len(invalid_rows)}\")\n",
        "print(invalid_rows.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byj4VFobq0ws",
        "outputId": "9ddf5c6c-8026-4eea-be32-7bb052d3b5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN values in magnetometer_z after conversion: 0\n",
            "Number of invalid rows in 'Mz': 0\n",
            "Empty DataFrame\n",
            "Columns: [participant_id, activity_position, Ax, Ay, Az, Lx, Ly, Lz, Gx, Gy, Gz, Mx, My, Mz, normalized, activity, position]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# **Load the dataset**\n",
        "data_path = '/content/drive/My Drive/subset_participant1_labeled.csv'\n",
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "\n",
        "# **Convert the magnetometer_z column to numeric, setting errors to NaN**\n",
        "df['Mz'] = pd.to_numeric(df['Mz'], errors='coerce')\n",
        "\n",
        "# **Drop rows with NaN in magnetometer_z**\n",
        "df.dropna(subset=['Mz'], inplace=True)\n",
        "\n",
        "# **Define feature columns for sensor readings**\n",
        "FEATURE_COLUMNS = ['Ax', 'Ay', 'Az',\n",
        "                   'Lx', 'Ly', 'Lz',\n",
        "                   'Gx', 'Gy', 'Gz',\n",
        "                   'Mx', 'My', 'Mz']\n",
        "\n",
        "# **Encode activity labels**\n",
        "label_encoder = LabelEncoder()\n",
        "df['activity_encoded'] = label_encoder.fit_transform(df['activity'])\n",
        "\n",
        "# **Define window size and overlap for creating sequences**\n",
        "WINDOW_SIZE = 2000\n",
        "OVERLAP_SIZE = 1000\n",
        "DOWNSAMPLE_FACTOR = 4\n",
        "\n",
        "# **Adjust window size for downsampling**\n",
        "DOWNSAMPLED_WINDOW_SIZE = WINDOW_SIZE // DOWNSAMPLE_FACTOR\n",
        "\n",
        "# **Create sequences of sensor data and labels**\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# **Create overlapping windows with downsampling**\n",
        "for start in range(0, len(df) - DOWNSAMPLED_WINDOW_SIZE * DOWNSAMPLE_FACTOR + 1, OVERLAP_SIZE):\n",
        "    end = start + DOWNSAMPLED_WINDOW_SIZE * DOWNSAMPLE_FACTOR\n",
        "    window = df.iloc[start:end:DOWNSAMPLE_FACTOR]  # Downsampling\n",
        "\n",
        "    # **Flatten the window data as a sequence of features (acts as input tokens for BERT)**\n",
        "    feature_sequence = window[FEATURE_COLUMNS].values.flatten()  # Creates a flat sequence for BERT input\n",
        "    label = window['activity_encoded'].mode()[0]  # Most common label in the window\n",
        "\n",
        "    X.append(feature_sequence)\n",
        "    y.append(label)\n",
        "\n",
        "# **Convert lists to numpy arrays**\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"Feature sequence shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG-KNHqlq7SH",
        "outputId": "a663295e-5d2e-4a32-d1af-783d18e40fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature sequence shape: (259, 6000)\n",
            "Labels shape: (259,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **Standardize the feature matrix `X`**\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"Standardized feature sequence shape: {X_standardized.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxWX-hJwsv3Z",
        "outputId": "10666c5d-7d24-4e77-afd2-5b70f4b47bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized feature sequence shape: (259, 6000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# **Define constants for reshaping and padding**\n",
        "NUM_FEATURES = 12  # Number of sensor features (Ax, Ay, Az, Lx, Ly, Lz, Gx, Gy, Gz, Mx, My, Mz)\n",
        "MAX_LENGTH = 512  # Maximum sequence length for BERT\n",
        "\n",
        "# **Calculate SEQUENCE_LENGTH based on NUM_FEATURES**\n",
        "# Ensure that SEQUENCE_LENGTH is an integer\n",
        "SEQUENCE_LENGTH = MAX_LENGTH // NUM_FEATURES\n",
        "\n",
        "# Adjust MAX_LENGTH to be exactly divisible by NUM_FEATURES\n",
        "MAX_LENGTH = SEQUENCE_LENGTH * NUM_FEATURES\n",
        "\n",
        "# **Convert `X_standardized` to a NumPy array and apply padding/truncation**\n",
        "X_standardized = np.array(X_standardized)\n",
        "\n",
        "# Padding or truncating standardized sequences to fit the max length (512)\n",
        "X_padded = np.array(\n",
        "    [\n",
        "        np.pad(seq, (0, max(0, MAX_LENGTH - len(seq))), 'constant')[:MAX_LENGTH]\n",
        "        for seq in X_standardized\n",
        "    ]\n",
        ")\n",
        "\n",
        "# **Convert to torch tensor and reshape to (num_samples, sequence_length, num_features)**\n",
        "X_padded = torch.tensor(X_padded, dtype=torch.float32).view(-1, SEQUENCE_LENGTH, NUM_FEATURES)\n",
        "\n",
        "# **Convert labels to tensor**\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# **Check the shape to ensure padding/truncating and reshaping worked correctly**\n",
        "print(f\"Padded and reshaped input shape: {X_padded.shape}, Labels shape: {y_tensor.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JB1VAc-sxng",
        "outputId": "bebf111d-2139-44cc-f613-f7dc99bd92f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded and reshaped input shape: torch.Size([259, 42, 12]), Labels shape: torch.Size([259])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **Create the Dataset and DataLoader**\n",
        "batch_size = 16\n",
        "dataset = TensorDataset(X_padded, y_tensor)\n",
        "\n",
        "# **Split dataset into training and testing sets (80% train, 20% test)**\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# **Data loaders**\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train DataLoader size: {len(train_loader)}, Test DataLoader size: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWqFpSGAtH15",
        "outputId": "f6a3dbb4-3d4f-45c6-c718-5ac9a8d40b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataLoader size: 13, Test DataLoader size: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# Define a custom BERT-based model\n",
        "class TimeSeriesBERT(nn.Module):\n",
        "    def __init__(self, num_classes, num_features):\n",
        "        super(TimeSeriesBERT, self).__init__()\n",
        "\n",
        "        # Define the BERT configuration\n",
        "        self.bert_config = BertConfig(\n",
        "            hidden_size=128,\n",
        "            num_attention_heads=4,\n",
        "            num_hidden_layers=4,\n",
        "            intermediate_size=256\n",
        "        )\n",
        "\n",
        "        # Initialize the BERT model\n",
        "        self.bert = BertModel(self.bert_config)\n",
        "\n",
        "        # Linear layer to map input features to BERT hidden size\n",
        "        self.linear_mapping = nn.Linear(num_features, 128)\n",
        "\n",
        "        # Classification layer\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear mapping to match BERT's hidden size\n",
        "        x = self.linear_mapping(x)\n",
        "\n",
        "        # Pass the input through BERT\n",
        "        bert_output = self.bert(inputs_embeds=x)['pooler_output']\n",
        "\n",
        "        # Classification layer\n",
        "        logits = self.classifier(bert_output)\n",
        "        return logits\n",
        "\n",
        "# Number of activity classes\n",
        "num_classes = len(torch.unique(y_tensor))\n",
        "\n",
        "# Initialize the model\n",
        "model = TimeSeriesBERT(num_classes=num_classes, num_features=12)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "D5A09yPLtNkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device).to(torch.float32)  # Move to device and ensure float32 type\n",
        "    labels = labels.to(device).to(torch.long)     # Move to device and ensure long type\n"
      ],
      "metadata": {
        "id": "QKolCJVItrLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Get all unique classes from the entire dataset\n",
        "all_classes = np.unique(y)  # Ensure all possible classes are considered\n",
        "train_labels = [y_tensor[i].item() for i in train_dataset.indices]  # Extract training labels\n",
        "\n",
        "# Compute class weights using all possible classes\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=all_classes, y=train_labels)\n",
        "\n",
        "# Convert to tensor and move to the appropriate device\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "# Define the loss function with class weights\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Gradient clipping\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "# Specify number of epochs for training\n",
        "num_epochs = 20\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9eNdHeDCts0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    # Apply Xavier initialization to Linear layers\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply Xavier initialization to Embedding layers (optional)\n",
        "    elif isinstance(m, nn.Embedding):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    # Apply Xavier initialization to LayerNorm layers (optional)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "        nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "# Apply weight initialization across the model\n",
        "model.apply(initialize_weights)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sf9R-jYt9VD",
        "outputId": "48994de3-0b0e-485b-d4a2-c5bcf34fc744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeSeriesBERT(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-3): 4 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=128, out_features=256, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (linear_mapping): Linear(in_features=12, out_features=128, bias=True)\n",
              "  (classifier): Linear(in_features=128, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace NaNs with the mean of each sequence\n",
        "for i in range(X_padded.size(0)):  # Iterate over each sequence\n",
        "    seq = X_padded[i]\n",
        "    nan_mask = torch.isnan(seq)\n",
        "    mean_value = seq[~nan_mask].mean() if torch.sum(~nan_mask) > 0 else 0\n",
        "    seq[nan_mask] = mean_value\n",
        "# Check for NaNs or infinite values in the dataset\n",
        "print(f\"Number of NaN values in X_padded: {torch.isnan(X_padded).sum().item()}\")\n",
        "print(f\"Number of infinite values in X_padded: {torch.isinf(X_padded).sum().item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFe3SpAWuAgZ",
        "outputId": "01381a71-ae66-48b5-bf37-3a57bb35581b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN values in X_padded: 0\n",
            "Number of infinite values in X_padded: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Unpack the batch and send data to device\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device).to(torch.float32)  # Ensure inputs are float32\n",
        "        labels = labels.to(device).to(torch.long)     # Ensure labels are long for classification\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "\n",
        "        max_grad_norm=1.0\n",
        "        # Clip gradients to prevent exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for this epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNpl7HVBuIWd",
        "outputId": "90b53ec2-2da7-4ab5-f701-24288e196c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 2.1990\n",
            "Epoch [2/20], Loss: 1.8125\n",
            "Epoch [3/20], Loss: 1.6336\n",
            "Epoch [4/20], Loss: 1.5847\n",
            "Epoch [5/20], Loss: 1.5064\n",
            "Epoch [6/20], Loss: 1.3976\n",
            "Epoch [7/20], Loss: 1.3552\n",
            "Epoch [8/20], Loss: 1.3612\n",
            "Epoch [9/20], Loss: 1.2933\n",
            "Epoch [10/20], Loss: 1.2705\n",
            "Epoch [11/20], Loss: 1.2511\n",
            "Epoch [12/20], Loss: 1.1966\n",
            "Epoch [13/20], Loss: 1.1763\n",
            "Epoch [14/20], Loss: 1.1620\n",
            "Epoch [15/20], Loss: 1.1524\n",
            "Epoch [16/20], Loss: 1.1150\n",
            "Epoch [17/20], Loss: 1.0774\n",
            "Epoch [18/20], Loss: 1.0685\n",
            "Epoch [19/20], Loss: 1.1079\n",
            "Epoch [20/20], Loss: 1.1117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store true and predicted labels\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        # Unpack the batch and move data to the device\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device).to(torch.float32)\n",
        "        labels = labels.to(device).to(torch.long)\n",
        "\n",
        "        # Forward pass to get predictions\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Get the predicted class (the index of the highest logit)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Store true and predicted labels\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the classification report (precision, recall, F1-score for each class)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Print confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eMwnObcuVuY",
        "outputId": "3c033dbd-3e0b-430b-c19a-05af8de382db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.1731\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            Biking       0.08      0.10      0.09        10\n",
            "           Running       1.00      1.00      1.00         3\n",
            "           Sitting       0.08      0.17      0.11         6\n",
            "          Standing       0.00      0.00      0.00        13\n",
            "           Walking       1.00      0.67      0.80         6\n",
            "Walking Downstairs       0.00      0.00      0.00         4\n",
            "  Walking Upstairs       0.00      0.00      0.00        10\n",
            "\n",
            "          accuracy                           0.17        52\n",
            "         macro avg       0.31      0.28      0.29        52\n",
            "      weighted avg       0.20      0.17      0.18        52\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1 0 3 3 0 2 1]\n",
            " [0 3 0 0 0 0 0]\n",
            " [1 0 1 2 0 1 1]\n",
            " [6 0 3 0 0 0 4]\n",
            " [0 0 1 1 4 0 0]\n",
            " [0 0 2 2 0 0 0]\n",
            " [4 0 2 3 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of true labels\n",
        "print(\"True label distribution:\")\n",
        "unique, counts = np.unique(y_true, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "# Check the distribution of predicted labels\n",
        "print(\"Predicted label distribution:\")\n",
        "unique_pred, counts_pred = np.unique(y_pred, return_counts=True)\n",
        "print(dict(zip(unique_pred, counts_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etMvTpjpucJw",
        "outputId": "f1463210-a4fb-443e-eb15-a3bf984fe120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True label distribution:\n",
            "{0: 10, 1: 3, 2: 6, 3: 13, 4: 6, 5: 4, 6: 10}\n",
            "Predicted label distribution:\n",
            "{0: 12, 1: 3, 2: 12, 3: 11, 4: 4, 5: 4, 6: 6}\n"
          ]
        }
      ]
    }
  ]
}